#!/bin/sh
#
# Copyright 2024 Chainguard, Inc.
# Author: Dustin Kirkland <kirkland@chainguard.dev>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# Establish our destination directory inside of the container where we run
cd /root
DEST_DIR="manpages"
PKG_DIR="packages"
mkdir -p "$DEST_DIR" "$PKG_DIR"

sync_local_package_archive() {
	# Ideally, we'd look at -doc packages only, but it seems that manpages
	# are often shipped in other binary packages, too
	# --doc-only seems to pick up about ~27K manpages, whereas the full run finds about 30K
	#PKGS=$(apk search | grep '\-doc\-' | sed -e "s/\-doc\-.*/\-doc/" | sort -u)
	#TOTAL=$(apk search | grep '\-doc\-' | sed -e "s/\-doc\-.*/\-doc/" | wc -l)
	#
	# So instead, we need to inspect all Wolfi manpages
	apk update
	# We need full wget, for -N option
	apk add wget
	# Get our full package list
	PKGS=$(apk search | sort -u)
	TOTAL=$(apk search | sort -u | wc -l)
	local apk= url=
	local a="x86_64"
	local all_done=0
	# Create a local package archive mirror
	for apk in $PKGS; do
		url="https://packages.wolfi.dev/os/$a/$apk.apk"
		[ -s "$PKG_DIR/$apk.apk" ] || wget -q --continue -N -O "$PKG_DIR/$apk.apk" "$url"
		# Should check signatures?
		# apk verify "$PKG_DIR/$apk.apk"
		all_done=$((all_done+1))
		echo "DONE SYNCING: [$all_done / $TOTAL] $((100 * all_done / TOTAL))%: $apk"
	done
}

get_all_manpages() {
	# FIXME: Sorted by name and version, last package wins, if there are namespace colisions
	local j=0 p=
	local total=$(ls "$PKG_DIR" | wc -l)
	# Need gnutar for tar --wildcards
	apk add gnutar
	for p in $PKG_DIR/*; do
		j=$((j+1))
		tar xf "$p" -C "$DEST_DIR" --wildcards "usr/share/man/*" --overwrite 2>/dev/null
		echo "DONE: [$j / $total] $((100 * j / total))%: $p"
	done
}


unzip_any_zipped_files() {
	# Unzip any files in $DEST_DIR, if there are any
	echo "INFO: Unzip any zipped files"
	find "$DEST_DIR" -type f -name "*.gz" | xargs -i gunzip -f {}
}

delete_non_manpages() {
	# Try to prune files that are likely not readable by the "man" command
	local f t
	echo "INFO: Deleting non manpages"
	apk add file
	for f in $(find "$DEST_DIR" -type f); do
		# Best guess at file type
		t=$(file "$f")
		case "$t" in
			*"troff or preprocessor input"*)
				# If 'file' says it's troff format, then good enough, it's a manpage
				continue
			;;
		esac
		if (basename "$f" | grep -q "\.[0-9]"); then
			# Looks like a manpage filename
			continue
		fi
		# Anything else, we'll scrap as not-a-manpage
		# Mostly these are htm, html, pdf, jpg, and gifs that leaked into /usr/share/man somehow...
		echo "$f" >> $DEST_DIR/deleted.index
		rm -f "$f"
	done
}

fix_permissions() {
	# Ensure permissions
	echo "INFO: Fixing permissions..."
	chown -R 1000:1000 $DEST_DIR
	find $DEST_DIR -type d | xargs -i chmod 755 {}
	find $DEST_DIR -type f | xargs -i chmod 644 {}
}

zip_each_file() {
	# gzip every manpage
	echo "INFO: Zipping all manpages..."
	find "$DEST_DIR" -type f \! -name \*.gz | xargs -i gzip -9 {}
}

fix_symlinks() {
	# Handle symlinks
	local dir dest
	echo "INFO: Updating symlinks to gzip files"
	for l in $(find "$DEST_DIR" -type l); do
		dir=$(dirname "$l")
		dest=$(readlink "$l")
		ln -sf "$dest".gz "$l.gz" && rm -f "$l"
	done
	# Delete broken links
	echo "INFO: Deleting broken links"
	find "$DEST_DIR" -type l ! -exec test -e {} \; -print | xargs -i rm -f {}
	find "$DEST_DIR" -type d -empty -delete
}

build_index() {
	# Build our manpage index
	echo "INFO: Building an index..."
	find "$DEST_DIR" | sed -e "s/.*manpages\///" | gzip -9 > "$DEST_DIR"/manpages.index.gz
}

create_archive() {
	# create a static archive for offline usage
	echo "INFO: Creating a static archive..."
	tar zcf "$DEST_DIR"/manpages.tar.gz "$DEST_DIR"
}

sync_local_package_archive	# ~5 hours for a full run, ~5 seconds for no-op; ~36G of disk
get_all_manpages		# ~22 minutes, and 260M of disk
unzip_any_zipped_files		# ~10 seconds
delete_non_manpages		# ~2 minutes
fix_permissions			# ~20 seconds
zip_each_file			# ~40 seconds
fix_symlinks			# ~40 seconds
build_index			# ~2 seconds
create_archive			# ~4 seconds

# Results in a ~46M manpages.tar.gz with ~25K manpages (97M uncompressed archive)
